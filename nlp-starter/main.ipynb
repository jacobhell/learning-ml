{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mediterranean-printer",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "Our training and test dataset were obtained from [this challenge](https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial?select=train.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "chinese-accident",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "unusual-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dynamic-currency",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-confidence",
   "metadata": {},
   "source": [
    "In our dataset, we have a dataframe of 4 columns. id which is the primary key, text, which is text from an author, and author, which is one of 3 values. \n",
    "\n",
    "**EAP** is Edgar Allen Poe\n",
    "**HPL** is H.P. Lovecraft\n",
    "**MWS** is Mary Shelley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aware-passage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19579, 3)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-iceland",
   "metadata": {},
   "source": [
    "When we look at the shape, we see we have 19579 lines of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "forbidden-export",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        0\n",
       "text      0\n",
       "author    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-proposal",
   "metadata": {},
   "source": [
    "Fortunately, there is no missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "false-illness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19579</td>\n",
       "      <td>19579</td>\n",
       "      <td>19579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>19579</td>\n",
       "      <td>19579</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>id24217</td>\n",
       "      <td>He declined bearing the cartel, however, and i...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text author\n",
       "count     19579                                              19579  19579\n",
       "unique    19579                                              19579      3\n",
       "top     id24217  He declined bearing the cartel, however, and i...    EAP\n",
       "freq          1                                                  1   7900"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "minimal-karma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EAP</th>\n",
       "      <td>7900</td>\n",
       "      <td>7900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HPL</th>\n",
       "      <td>5635</td>\n",
       "      <td>5635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MWS</th>\n",
       "      <td>6044</td>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  text\n",
       "author            \n",
       "EAP     7900  7900\n",
       "HPL     5635  5635\n",
       "MWS     6044  6044"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby('author').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-shift",
   "metadata": {},
   "source": [
    "It looks like Edgar Allen Poe's work has the most entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-alaska",
   "metadata": {},
   "source": [
    "## NLP\n",
    "\n",
    "There's 4 steps to preprocessing text for NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-rotation",
   "metadata": {},
   "source": [
    "1. Tokenization - Segregation of the text into its individual constitutent words.\n",
    "2. Stopwords - Throw away any words that occur too frequently as its frequency of occurrence will not be useful in helping detecting relevant texts. (as an aside also consider throwing away words that occur very infrequently).\n",
    "3. Lemmatization - combine variants of words into a single parent word that still conveys the same meaning\n",
    "4. Vectorization - Converting text into vector format. One of the simplest is the famous bag-of-words approach, where you create a matrix (for each document or text in the corpus). In the simplest form, this matrix stores word frequencies (word counts) and is oft referred to as vectorization of the raw text.\n",
    "\n",
    "### 1. Tokenization\n",
    "\n",
    "Tokenization will make a list of words and punctuation present in a corpus. Here is how you do it using nltk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ahead-junior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['process',\n",
       " ',',\n",
       " 'however',\n",
       " ',',\n",
       " 'afforded',\n",
       " 'me',\n",
       " 'no',\n",
       " 'means',\n",
       " 'of',\n",
       " 'ascertaining',\n",
       " 'the',\n",
       " 'dimensions',\n",
       " 'of',\n",
       " 'my',\n",
       " 'dungeon',\n",
       " ';',\n",
       " 'as',\n",
       " 'I',\n",
       " 'might']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_text = train.text.values[0]\n",
    "print(first_text)\n",
    "print(\"=\"*90)\n",
    "first_text_list = nltk.word_tokenize(first_text)\n",
    "first_text_list[1:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-bouquet",
   "metadata": {},
   "source": [
    "## 2. Stopwords\n",
    "\n",
    "Stopwords will remove words like 'the' and 'to', which are very common in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "foreign-moment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " 'however',\n",
       " ',',\n",
       " 'afforded',\n",
       " 'means',\n",
       " 'ascertaining',\n",
       " 'dimensions',\n",
       " 'dungeon',\n",
       " ';',\n",
       " 'might',\n",
       " 'make',\n",
       " 'circuit',\n",
       " ',',\n",
       " 'return',\n",
       " 'point',\n",
       " 'whence',\n",
       " 'set',\n",
       " ',',\n",
       " 'without']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "first_text_list_cleaned = [word for word in first_text_list if word.lower() not in stopwords]\n",
    "first_text_list_cleaned[1:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-operator",
   "metadata": {},
   "source": [
    "### 3. Lemmatization \n",
    "The work at this stage attempts to reduce as many different variations of similar words into a single term.\n",
    "\n",
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "unnecessary-belgium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lemmatized form of leaves is: leaf\n"
     ]
    }
   ],
   "source": [
    "lemm = WordNetLemmatizer()\n",
    "print(\"The lemmatized form of leaves is: {}\".format(lemm.lemmatize(\"leaves\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-quest",
   "metadata": {},
   "source": [
    "### 4. Vectorization\n",
    "\n",
    "Lastly, we want to vectorize the text. We are going to use the Bag of Words approach to change a sentence into a list of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "minor-object",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features are:\n",
      " ['burgers', 'eat', 'fries', 'love', 'to']\n",
      "\n",
      "The vectorized array looks like:\n",
      " [[1 1 0 1 1]\n",
      " [0 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"I love to eat Burgers\", \n",
    "            \"I love to eat Fries\"]\n",
    "vectorizer = CountVectorizer(min_df=0)\n",
    "sentence_transform = vectorizer.fit_transform(sentence)\n",
    "print(\"The features are:\\n {}\".format(vectorizer.get_feature_names()))\n",
    "print(\"\\nThe vectorized array looks like:\\n {}\".format(sentence_transform.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-hello",
   "metadata": {},
   "source": [
    "## Topic Modelling\n",
    "\n",
    "Getting back to the two techniques:\n",
    "\n",
    "1. LDA: Assigns weights to words in a corpus, where each topic will assign different probability weights to each word.\n",
    "2. NMF: Takes an input matrix and approximates the factorization of this matrix into two other matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "electronic-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        lemm = WordNetLemmatizer()\n",
    "        analyzer = super(LemmaCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (lemm.lemmatize(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "heavy-james",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the entire training text in a list\n",
    "text = list(train.text.values)\n",
    "# Calling our overwritten Count vectorizer\n",
    "tf_vectorizer = LemmaCountVectorizer(max_df=0.95, \n",
    "                                     min_df=2,\n",
    "                                     stop_words='english',\n",
    "                                     decode_error='ignore')\n",
    "tf = tf_vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "tender-copyright",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(learning_method='online', learning_offset=50.0,\n",
       "                          max_iter=5, n_components=11, random_state=0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=11, max_iter=5,\n",
    "                                learning_method = 'online',\n",
    "                                learning_offset = 50.,\n",
    "                                random_state = 0)\n",
    "\n",
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "single-point",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for index, topic in enumerate(model.components_):\n",
    "        message = \"\\nTopic #{}:\".format(index)\n",
    "        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1 :-1]])\n",
    "        print(message)\n",
    "        print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "vulnerable-digest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model: \n",
      "\n",
      "Topic #0:mean night fact young return great human looking wonder countenance difficulty greater wife finally set possessed regard struck perceived act society law health key fearful mr exceedingly evidence carried home write lady various recall accident force poet neck conduct investigation\n",
      "======================================================================\n",
      "\n",
      "Topic #1:death love raymond hope heart word child went time good man ground evil long misery replied filled passion bed till happiness memory heavy region year escape spirit grief visit doe story beauty die plague making influence thou letter appeared power\n",
      "======================================================================\n",
      "\n",
      "Topic #2:left let hand said took say little length body air secret gave right having great arm thousand character minute foot true self gentleman pleasure box clock discovered point sought pain nearly case best mere course manner balloon fear head going\n",
      "======================================================================\n",
      "\n",
      "Topic #3:called sense table suddenly sympathy machine sens unusual labour thrown mist solution suppose specie movement whispered urged frequent wine hour appears ring turk place stage noon justine ceased obscure chair completely exist sitting supply weird bottle seated drink material bell\n",
      "======================================================================\n",
      "\n",
      "Topic #4:house man old soon city room sight did believe mr light entered sir cloud order ill way dr apparently clear certain forgotten day quite door considered need great fine began journey search walked disposition view long concerning walk drawn saw\n",
      "======================================================================\n",
      "\n",
      "Topic #5:thing thought eye mind said men night like face life head dream knew saw form world away deep stone told matter morning perdita dead general man strange seen terrible sleep tell object tear know account better black say remained little\n",
      "======================================================================\n",
      "\n",
      "Topic #6:father moon stood longer attention end sure leave remember time excited period trace dream given star place able grew subject set cut visited captain consequence marie taking forward started descent atmosphere impulse departure dog men truly abyss appear magnificent quarter\n",
      "======================================================================\n",
      "\n",
      "Topic #7:day did heard life time friend new far horror nature come look tree year present soul passed known people heart felt degree scene idea hand feeling world came country adrian moment make word affection sun gone reached idris youth seen\n",
      "======================================================================\n",
      "\n",
      "Topic #8:came earth street near like sound wall window just open lay fell wind looked saw moment water eye dark spirit beneath mountain old did light foot long town space floor low happy held half voice living direction ear small end\n",
      "======================================================================\n",
      "\n",
      "Topic #9:shall place sea time think long fear know mother day person say brought expression land change question night result ye week mad month feel god rest got manner course horrible large resolved kind passage far discovery word answer eye ago\n",
      "======================================================================\n",
      "\n",
      "Topic #10:door turned close away design view doubt ordinary tried oh madness room enemy le lower exertion chamber opening candle legend occupation abode lofty author compartment breath flame accursed machinery horse iron proceeded curse ve louder desired entering appeared lock oil\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in LDA model: \")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-startup",
   "metadata": {},
   "source": [
    "And that's how you do it. I'm not entirely sure how it works yet either, but I am learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn-2",
   "language": "python",
   "name": "nn-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
